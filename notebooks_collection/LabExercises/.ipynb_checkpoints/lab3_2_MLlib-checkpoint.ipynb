{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"parkinsons_speech.jpg\" width=\"500\">\n",
    "\n",
    "## Linear regression\n",
    "\n",
    "Today's second exercise will involve linear regression with stochastic gradient descent and with <a href=\"http://spark.apache.org/docs/latest/mllib-guide.html\">MLlib</a>.\n",
    "\n",
    "This exercise will be divided into three parts:\n",
    "\n",
    "+ #### 1. Importing and preparing the data\n",
    "+ #### 2. Creating a baseline benchmark\n",
    "+ #### 3. Utilizing MLlib\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "In the following exercises you will need to replace the code parts in the cell that starts with following comment: \"#Replace the `<INSERT>`\"\n",
    "\n",
    "To go through the notebook fill in the `<INSERT>`:s with appropriate code in the cells. \n",
    "To run a cell press Shift-Enter to run it and advance to the following cell or Ctrl-Enter to only run the code in the cell. You should do the exercises from the top to the bottom in this notebook, because following cells may depend on code in previous cells.\n",
    "\n",
    "## Description of the data set\n",
    "In this exercise we will utilize the <a href=\"http://archive.ics.uci.edu/ml/datasets/Parkinsons+Telemonitoring\">Parkinsons Telemonitoring Data Set</a> used in the following article <a href=\"http://www.cabdyn.ox.ac.uk/complexity_PDFs/Network%20Journal%20Club%20PDFs/athanasios.pdf\">Accurate telemonitoring of Parkinson’s disease progression by non-invasive speech tests</a>:\n",
    "\n",
    ">Tracking Parkinson's disease (PD) symptom progression often uses the Unified Parkinson’s Disease Rating Scale (UPDRS), which requires the patient's presence in clinic, and time-consuming physical examinations by trained medical staff. Thus, symptom monitoring is costly and logistically inconvenient for patient and clinical staff alike, also hindering recruitment for future large-scale clinical trials. [...] We characterize speech with signal processing  algorithms, and statistically map these algorithms to UPDRS. We verify our findings on the largest database of PD speech in existence (~6,000 recordings from 42 PD patients, recruited to a six-month, multi-centre trial).\n",
    "\n",
    "In essence, we will try to accurately predict a person's Parkinson's progression, meassured on 176 point scale (UPDRS) based on vocal features from the patients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing and preparing the data\n",
    "The data set is currently saved as a text file in the <a href=\"https://en.wikipedia.org/wiki/Comma-separated_values\">comma separated values (CSV)</a> format, without a header, named \"parkinsons_updrs.csv\". We want to read in this textfile as separate lines into an rdd. A line is represented by the following column headers:\n",
    "\n",
    "<b>UPDRS <br>\n",
    "age <br>\n",
    "sex <br>\n",
    "test_time  <br>\n",
    "Jitter(Abs) <br>\n",
    "Jitter:RAP <br>\n",
    "Shimmer:APQ3 <br>\n",
    "Shimmer:DDA\t<br>\n",
    "Jitter(%) <br>\n",
    "Jitter:PPQ5\t<br>\n",
    "Jitter:DDP <br>\n",
    "Shimmer <br>\n",
    "Shimmer(dB) <br>\n",
    "Shimmer:APQ5 <br>\n",
    "Shimmer:APQ11 <br>\n",
    "NHR <br>\n",
    "HNR <br>\n",
    "RPDE <br>\n",
    "DFA <br>\n",
    "PPE</b>\n",
    "\n",
    "Where UDPRS is the label of the observation (data point), which is a float between 0 and 176. The following 19 columns are numeric features.\n",
    "\n",
    "If we want to run these lines as part of a script we should create a spark context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from pyspark import SparkContext, StorageLevel\n",
    "#from pyspark.sql import SQLContext\n",
    "#sc = SparkContext(master=\"local[*]\")\n",
    "#sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Store the parkinsons_updrs.csv in hdfs and read it after as RDD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -put \"./parkinsons_updrs.csv\" \"parkinsons_updrs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the <INSERT>\n",
    "rawLines = sc.<INSERT>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Check if the RDD is correct\n",
    "\n",
    "Utilize the <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.count\">count method</a> to check how many observations there are in the RDD. Then use the <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take\">take method</a> to take the first observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the <INSERT>\n",
    "numObs = <INSERT>\n",
    "print \"The number of observations: \"+str(numObs)\n",
    "sampleObs = <INSERT>\n",
    "sampleObs = sampleObs[0]\n",
    "numFeatures = len(sampleObs.split(','))-1\n",
    "print \"The number of features: \"+str(numFeatures)\n",
    "print \"One observation: \"+ str(sampleObs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions to check results\n",
    "def check(x,y,label):\n",
    "    if(x == y):\n",
    "        print(\"Yay, \"+label+\" is correct!\")\n",
    "    else:\n",
    "        print(\"Nay, \"+label+\" is incorrect, please try again!\")\n",
    "\n",
    "def checkArray(x,y,label):\n",
    "    if np.allclose(x,y):\n",
    "        print(\"Yay, \"+label+\" is correct!\")\n",
    "    else:\n",
    "        print(\"Nay, \"+label+\" is incorrect, please try again!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the observations are correct\n",
    "check(numObs, 5875, \"the number of observations\")\n",
    "check(numFeatures, 19, \"the number of features\")\n",
    "check(len(sampleObs), 143, \"the first observation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Using MLlib's LabeledPoint\n",
    "In the MLlib library, an observation is represented by a <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint\">LabeledPoint</a>, which consists of label, a double representing the value for regression purposes, or an integer value representing the class for classification purposes: 0 (negative) and 1 (positive) for binary classification and 0,1,2,... for multiclass. To accompany the label, there is also an feature vector, called features, which consists of numeric features stored as either a list, NumPy array, pyspark.mllib.linalg.SparseVector, or scipy.sparse column matrix. \n",
    "\n",
    "Implement the parseObs function below that takes a comma separated string and created a labeled point out of it using the following function: <a href=\"https://docs.python.org/2/library/string.html#string.split\">string.split()</a>, create a parsed observation of sampleObs and print the label and features of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import some stuff\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the <INSERT>\n",
    "def parseObs(line):\n",
    "    \"\"\"Parses a comma separated string into a LabeledPoint.\n",
    "\n",
    "    Args:\n",
    "         line (string): A comma separated string where the first element is the label and the \n",
    "                        following elements  are the features.\n",
    "\n",
    "    Returns:\n",
    "            LabeledPoint: The line is parsed into a LabeledPoint, which has a label and features.\n",
    "    \"\"\"\n",
    "    return <INSERT>\n",
    "\n",
    "paresedObs = parseObs(sampleObs)\n",
    "ObsLabel = <INSERT>\n",
    "ObsFeatures = <INSERT>\n",
    "print(ObsLabel, ObsFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the LabeledPoint is correct\n",
    "check(ObsLabel,34.398,\"the label of the first observation\")\n",
    "check(len(ObsFeatures),19,\"the features of the first observation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Parsing the observations\n",
    "Parse the points in rawLines into LabeledPoints using the above defined function parseObs, then create two RDD:s, one only containing the labels and one containing only features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parsedObsInit = rawLines.map(lambda x: parseObs(x))\n",
    "features = parsedObsInit.map(lambda x: x.features)\n",
    "labels = parsedObsInit.map(lambda x: x.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the parsed labels and features are correct\n",
    "check(features.collect()[5874][18],0.15336,\"the features vector\")\n",
    "check(labels.collect()[5874],31.513,\"the labels list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Change the labels to start in origin\n",
    "\n",
    "Let us first examine the range of labels (should be contained within the range $[0,176]$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minYear = labels.reduce(min)\n",
    "maxYear = labels.reduce(max)\n",
    "print(minYear,maxYear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regression tasks, it is often usual to change labels such that they begin in origin. Convert parsedObsInit to a new RDD consisting of LabeledPoint:s with the labels changed such as that smallest label equals zero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsedData = parsedObsInit.map(lambda x: LabeledPoint(x.label - minYear, x.features))\n",
    "labels = parsedData.map(lambda x: x.label)\n",
    "minYearNew = labels.reduce(min)\n",
    "maxYearNew = labels.reduce(max)\n",
    "print(minYearNew, maxYearNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the change in labels is correct\n",
    "check(labels.take(1)[0],27.398000000000003,\"the change in labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Summary statistics of the features\n",
    "Before starting a ML task, you should take a look at the features, and a statistical summary can be a good start: \n",
    "<a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.Statistics.colStats\">colStats()</a> returns an instance of <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.stat.MultivariateStatisticalSummary\">MultivariateStatisticalSummary</a>\n",
    ", which contains the column-wise max, min, mean, variance, and number of nonzeros, as well as the total count.\n",
    "\n",
    "The data set we are currently working on, is a well-behaved data set, we got no missing values and no extreme minimum or maximum values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "summary = Statistics.colStats(features)\n",
    "print(\"The number of non zero features:\\n\"+str(summary.numNonzeros()))\n",
    "print(\"The min of the features:\\n\"+str(summary.min()))\n",
    "print(\"The max of the features:\\n\"+str(summary.max()))\n",
    "print(\"The mean of the features:\\n\"+str(summary.mean()))\n",
    "print(\"The variance of the features:\\n\"+str(summary.variance()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Scaling and normalizing the observations\n",
    "In the MLlibrary there exist two classes representing <a href=\"https://en.wikipedia.org/wiki/Feature_scaling\">feature scaling</a> which are the\n",
    "<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.feature.StandardScalerModel\">StandardScaler</a> and the <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.feature.Normalizer\">Normalizer</a>. The StandardScaler can take an input of RDD[Vector], learn the summary statistics, and then return a model which can transform the input dataset into unit standard deviation and/or zero mean features, i.e. taking the $x'$ of each feature. The Normalizer scales individual feature vectors to have unit p-norm. i.e. calculating $\\mathbf{v'}$.\n",
    "\n",
    "$$\n",
    "x' = \\frac{x-\\hat{x}}{\\sigma}\n",
    "\\qquad\n",
    "\\|\\mathbf{v}\\|_p = (\\sum\\limits_{i=1}^n |x_i|^p)^{1/p}\n",
    "\\qquad\n",
    "\\mathbf{v'} = \\frac{\\mathbf{v}}{\\|\\mathbf{v}\\|_p}\n",
    "$$\n",
    "The following page contains examples of feature scaling and normalizing <a href=\"http://spark.apache.org/docs/latest/mllib-feature-extraction.html#model-fitting\">examples</a>. Scale the features with the standard deviation, but not the mean and then normalize the scaled features with the $p=\\infty$ norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import some more stuff\n",
    "from pyspark.mllib.feature import StandardScaler\n",
    "from pyspark.mllib.feature import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the <INSERT>\n",
    "# Check in the slides the methods to use\n",
    "scaler = <INSERT>\n",
    "scaledFeatures = scaler.<INSERT>\n",
    "norm = <INSERT>\n",
    "normScaledFeatures = norm.<INSERT>\n",
    "normScaledObs = labels.zip(normScaledFeatures)\n",
    "normScaledObs  = normScaledObs.map(lambda x: LabeledPoint(x[0], x[1]))\n",
    "print(normScaledObs.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check(scaledFeatures.take(1)[0][18],1.7493184105028705,\"the scaled features\")\n",
    "check(normScaledFeatures.take(1)[0][18],0.21432852613822129,\"the normalized and scaled features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.8 Illustrating the features\n",
    "To try to find patterns in the features, we will visualize the data set, by projecting it from 19 dimensions to 2. This dimensionality reduction is achieved by using <a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\">PCA</a>, a type of unsupervised learning, which we will explain further next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "#Calculate the PCA of normScaledFeatures\n",
    "Y = TruncatedSVD(n_components=2).fit_transform(normScaledFeatures.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#To inline the plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=labels.collect(),linewidths=.5, s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the graph above, you probably see some regions that look like clusters, and others where the colors are intermingled. The color map associates lower values to blue, and higher values to red; values in middle are in green. The intermingled points makes this a hard problem, to try to find a linear regression that correctly predicts the labels will not be an easy task. \n",
    "\n",
    "### 1.9  Splitting the data into training, validation, and test sets \n",
    "Our last task with preparing the data set is to split it into a training, validation and test set. An usual split is 70%/15%/15%.\n",
    "\n",
    "Utilize the <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit\">randomSplit</a> function, with the weigths and seed specified below. Then cache the three data sets, because you will reuse them during this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the <INSERT>\n",
    "weights = [.7, .15, .15]\n",
    "seed = 0\n",
    "trainObs, valObs, testObs = normScaledObs.<INSERT>\n",
    "\n",
    "trainObs.<INSERT>\n",
    "valObs.<INSERT>\n",
    "testObs.<INSERT>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrain = trainObs.count()\n",
    "numVal = valObs.count()\n",
    "numTest = testObs.count()\n",
    "\n",
    "print(\"Size of train set: \"+str(numTrain))\n",
    "print(\"Size of validation set: \"+str(numVal))\n",
    "print(\"Size of test set: \"+str(numTest))\n",
    "print(\"Size of total data set: \"+str(+numTrain+numVal+numTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the data sets are correct\n",
    "check(numTrain, 4089, \"the train set\")\n",
    "check(numVal, 877, \"the validation set\")\n",
    "check(numTest, 909, \"the test set\")\n",
    "check(numTrain+numVal+numTest, 5875, \"the total data set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating a baseline benchmark\n",
    "### 2.1 The average UPDRS score\n",
    "An quite natural baseline to compare your result against is to predict the same value for each observation, using the average value.\n",
    "\n",
    "Compute the average UPDRS score (the label) below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanUPDRS = trainObs.map(lambda x: x.label).mean() \n",
    "print(meanUPDRS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the average UPDRS is correct\n",
    "check(round(meanUPDRS),22.0, \"the average UPDRS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The mean absolute error\n",
    "For evaluation purposes, we will use the <a href=\"https://en.wikipedia.org/wiki/Mean_absolute_error\"> mean absolute error</a> (MAE) as a metric. The MAE is defined as the sum of the absolute value off the label ($y_i$) minus the predicted value ($\\hat{y}_i$), divided by the number of labels:\n",
    "$$\n",
    "\\frac{1}{n}\\sum\\limits_{i=1}^n |y_i-\\hat{y}_i|\n",
    "$$\n",
    "\n",
    "Implement the function below that calculates the mean absolute error for a RDD of (label, prediction) tuples. You will use map() and the mean() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace the <INSERT>\n",
    "def calcMAE(labelPred):\n",
    "    \"\"\"Computes the mean absolute error for a RDD of (label, prediction) tuples.\n",
    "\n",
    "    Args:\n",
    "        labelPred (RDD of (float, float)): A RDD consisting of (label, prediction) tuples.\n",
    "\n",
    "    Returns:\n",
    "        float: The mean absolute error.\n",
    "    \"\"\"\n",
    "    meanSumDiff = labelPred.<INSERT>\n",
    "    return meanSumDiff\n",
    "\n",
    "\n",
    "labelPred = sc.parallelize([(4., 1.), (2., 1.), (17., 17.)])\n",
    "# RMSE = sqrt[|4-1| + |2-1| + |17-17|) / 3] = 1.33333333333\n",
    "testMAE = calcMAE(labelPred)\n",
    "print(testMAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check(round(testMAE,2),1.33, \"the calculate MAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The MAE of the train, validation, and test set\n",
    "We will now calculate the baseline benchmark MAE value for all three data sets.\n",
    "\n",
    "In the code below please compute the train, validation, and test benchmark MAE, by first creating a pair RDD consisting of (label, averageUPDRS) and then using the above defined function to calculate the MAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainLabelPred = trainObs.map(lambda x: (x.label, meanUPDRS))\n",
    "trainBenchMAE = calcMAE(trainLabelPred)\n",
    "\n",
    "trainLabelPred = valObs.map(lambda x: (x.label, meanUPDRS))\n",
    "valBenchMAE = calcMAE(trainLabelPred)\n",
    "\n",
    "testLabelPred = testObs.map(lambda x: (x.label, meanUPDRS))\n",
    "testBenchMAE = calcMAE(testLabelPred)\n",
    "\n",
    "print('Train benchmark MAE = {0:.3f}'.format(trainBenchMAE))\n",
    "print('Validation benchmark MAE = {0:.3f}'.format(valBenchMAE))\n",
    "print('Test benchmark MAE = {0:.3f}'.format(testBenchMAE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check(round(trainBenchMAE,3),8.585, \"the train benchmark MAE\")\n",
    "check(round(valBenchMAE,3),8.768, \"the validation benchmark MAE\")\n",
    "check(round(testBenchMAE,3),8.862, \"the test benchmark MAE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of linear regression with gradient descent is about 0.305 better then the benchmark. It is an improvement compared to the benchmark, but not a huge difference. This means that this is a hard task to get good result with linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utilizing MLlib\n",
    "\n",
    "### 3.1 Linear regression with stochastic gradient descent\n",
    "The MLlib's <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionWithSGD\">LinearRegressionWithSGD</a> uses a stochastic gradient approximation, allowing for L1 or L2 regularization, and including an intercept in the model.\n",
    "\n",
    "Implement the code below, use the LinearRegressionWithSGD to train a model with the iterations, step size, reguralization parameter set to the below values, use the default values for the rest. This method returns a <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionModel\">LinearRegressionModel</a>, which you can use to predict LabeledPoint:s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Even more stuff imported!\n",
    "from pyspark.mllib.regression import LinearRegressionWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Replace the <INSERT>\n",
    "iters = 20  # iterations\n",
    "alpha = 1.0  # step\n",
    "reg = 1e-4  # regParam\n",
    "linRegSGD = LinearRegressionWithSGD.<INSERT>\n",
    "\n",
    "#Get the weigths of the model\n",
    "weightsLRSGD = linRegSGD.weights\n",
    "print (weightsLRSGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkArray(weightsLRSGD, [8.30193710199,-0.33530190475,2.04977378005,1.09952121318,1.24159481483,\n",
    "                          0.936720117339,0.772573966266,0.936817553234,1.16986198426,1.23938528482,\n",
    "                          1.11149306596,1.03967181129,1.48218083935,1.11151636353,0.247489223328,\n",
    "                          3.49634026933,5.25804171983,7.79639148963,3.00069009988]\n",
    "          ,\"the weights of the model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluate the Linear regression with SGD\n",
    "We will check the new model on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelPred = valObs.map(lambda lp: (lp.label, linRegSGD.predict(lp.features)))\n",
    "valLRSGDMAE = calcMAE(labelPred)\n",
    "\n",
    "print(\"Validation MAE:\\n\\tBenchmark: = {0:.3f}\\n\\tLRSGD = {1:.3f}\"\n",
    "      .format(valBenchMAE, valLRSGDMAE))\n",
    "print(\"\\nThe difference between Benchmark and SGD = {0:.3f}\".format(valBenchMAE-valLRSGDMAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Improve the hyperparameters\n",
    "All the options to LinearRegressionWithSGD can be seen as hyperparameters: iterations, step size, mini batch fraction, regularization type, reguralization parameter, and intercept. For this task the most important parameters to tune is the iterations, and step size options. So we will do it in the following cells, often if you have reasonable values for the other parameters, you can tune one parameter in isolation.\n",
    "\n",
    "So run the following cells to find optimal values for the two parameters:\n",
    "\n",
    "#### 4.3.1 Improve the iterations parameter\n",
    "We will try several values for the iteration parameter and pick the one that gets the best result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestMAE = valLRSGDMAE\n",
    "bestIters = 0\n",
    "for iters in [10,50,100,500,1000]:\n",
    "    model = LinearRegressionWithSGD.train(trainObs, iterations=iters, step=alpha, regParam=reg)\n",
    "    labelPred = valObs.map(lambda lp: (lp.label, model.predict(lp.features)))\n",
    "    valMAE = calcMAE(labelPred)\n",
    "        \n",
    "    if valMAE < bestMAE:\n",
    "        bestMAE = valMAE\n",
    "        bestIters = iters\n",
    "        \n",
    "    print(\"iters = {0}, MAE = {1:.3f}\".format(iters, valMAE))\n",
    "        \n",
    "print(\"Best MAE for the following value of iters = {0}, MAE = {1:.3f}\".\n",
    "      format(bestIters, bestMAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Improve the step size parameter (alpha)\n",
    "We will try some values for the step size parameter (this may take while on a slow computer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestMAE = valLRSGDMAE\n",
    "bestAlpha = 0\n",
    "for alpha in np.arange(1,6,1):\n",
    "    model = LinearRegressionWithSGD.train(trainObs, iterations=bestIters, step=alpha,regParam=reg)\n",
    "    labelPred = valObs.map(lambda lp: (lp.label, model.predict(lp.features)))\n",
    "    valMAE = calcMAE(labelPred)\n",
    "        \n",
    "    if valMAE < bestMAE:\n",
    "        bestMAE = valMAE\n",
    "        bestAlpha = alpha\n",
    "        \n",
    "    print(\"alpha = {0}, MAE = {1:.3f}\".format(alpha, valMAE))\n",
    "        \n",
    "print(\"Best MAE for the following value of alpha = {0}, MAE = {1:.3f}\".\n",
    "      format(bestAlpha, bestMAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Evaluate the linear regression with SGD and improved hyperparameters\n",
    "Let use evaluate the new model with the optimal parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((\"Validation MAE:\\n\\tBenchmark: = {0:.3f}\\n\\tLRSGD = {1:.3f}\"+\n",
    "      \"\\n\\tLRSGDOpt = {2:.3f}\").format(valBenchMAE, valLRSGDMAE, bestMAE))\n",
    "print(\"\\nThe difference between Benchmark and SGDOpt = {0:.3f}\".format(valBenchMAE-bestMAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the benchmark and the model with optimal parameters is 0.626, almost the double of the value that we got in part 3.4 of this exercise. A good relative increase, although not that great in absolute terms, but again this is hard task to do linear regression on.\n",
    "\n",
    "### 3.5 Evaluate all the models on the test set\n",
    "And finally we will evaluate all the models on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Calculate the test MAE for linear regressions with SGD\n",
    "labelPred = testObs.map(lambda lp: (lp.label, linRegSGD.predict(lp.features)))\n",
    "testLRSGDMAE = calcMAE(labelPred)\n",
    "\n",
    "#Calculate the test MAE for linear regressions with SGD, with (optimal) hyperparameters\n",
    "model = LinearRegressionWithSGD.train(trainObs, iterations=bestIters, step=bestAlpha,\n",
    "                                          regParam=reg, intercept=False)\n",
    "labelPred = testObs.map(lambda lp: (lp.label, model.predict(lp.features)))\n",
    "testOptMAE = calcMAE(labelPred)\n",
    "\n",
    "print((\"Test MAE:\\n\\tBenchmark: = {0:.3f}\\n\\tLRSGD = {1:.3f}\"+\n",
    "      \"\\n\\tLRSGDOpt = {2:.3f}\").format(testBenchMAE, testLRSGDMAE, \n",
    "                                       testOptMAE))\n",
    "print(\"\\nThe difference between Benchmark and SGDOpt = {0:.3f}\".format(testBenchMAE-testOptMAE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Post mortem discussion\n",
    "To have something to compare with, we can look at the original article <a href=\"http://www.cabdyn.ox.ac.uk/complexity_PDFs/Network%20Journal%20Club%20PDFs/athanasios.pdf\">Accurate telemonitoring of Parkinson’s disease progression by non-invasive speech tests</a> at the last page we can find the best result for two types of regression, one linear: <a href=\"https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares\">Iteratively reweighted least squares</a> and one non-linear: <a href=\"https://en.wikipedia.org/wiki/Decision_tree_learning#Types\">classification and regression tree</a>.\n",
    "\n",
    "The linear ones best result is a MAE of 8.46, but this result is on a mean of 1000 runs of 10-fold cross validation, so it is not directly comparable, but can give an indication of the quality of the result. The non-linear method has a result of 7.52, i.e. almost one point lower, which indicates that this task is non-linear in it's nature.\n",
    "\n",
    "As an optional home assignment you will implement the gradient descent yourself."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
